{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c3dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcac416",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a09d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def robust_temporal_split(df, user_col='customer_id', time_col='order_day',\n",
    "                         test_size=0.2, min_test_orders=2, val_size=0.1):\n",
    "    \"\"\"Improved temporal split with proper validation set\"\"\"\n",
    "    df_sorted = df.sort_values([user_col, time_col])\n",
    "    splits = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "    for user_id in df_sorted[user_col].unique():\n",
    "        user_orders = df_sorted[df_sorted[user_col] == user_id]\n",
    "\n",
    "        if len(user_orders) <= min_test_orders + 2:\n",
    "            # Small user: use last order for test, previous for val\n",
    "            train_val = user_orders.iloc[:-1]\n",
    "            test = user_orders.iloc[-1:]\n",
    "            train = train_val.iloc[:-1]\n",
    "            val = train_val.iloc[-1:]\n",
    "        else:\n",
    "            # Proper split with validation\n",
    "            split_idx = max(min_test_orders, int(len(user_orders) * (1 - test_size)))\n",
    "            val_idx = max(1, int(split_idx * (1 - val_size)))\n",
    "\n",
    "            train = user_orders.iloc[:val_idx]\n",
    "            val = user_orders.iloc[val_idx:split_idx]\n",
    "            test = user_orders.iloc[split_idx:]\n",
    "\n",
    "        splits['train'].append(train)\n",
    "        splits['val'].append(val)\n",
    "        splits['test'].append(test)\n",
    "\n",
    "    return {k: pd.concat(v).reset_index(drop=True) for k, v in splits.items()}\n",
    "\n",
    "def create_interaction_matrix(df, user_col='customer_id', item_col='vendor_id'):\n",
    "    \"\"\"Create user-item interaction matrix for collaborative filtering\"\"\"\n",
    "    # Create categorical codes\n",
    "    user_codes = df[user_col].astype('category').cat.codes\n",
    "    item_codes = df[item_col].astype('category').cat.codes\n",
    "\n",
    "    # Get the unique users and items\n",
    "    unique_users = df[user_col].unique()\n",
    "    unique_items = df[item_col].unique()\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    user_map = {code: user_id for code, user_id in enumerate(df[user_col].astype('category').cat.categories)}\n",
    "    item_map = {code: item_id for code, item_id in enumerate(df[item_col].astype('category').cat.categories)}\n",
    "\n",
    "    # Create reverse mappings\n",
    "    reverse_user_map = {user_id: code for code, user_id in user_map.items()}\n",
    "    reverse_item_map = {item_id: code for code, item_id in item_map.items()}\n",
    "\n",
    "    # Create sparse matrix\n",
    "    interaction_matrix = csr_matrix(\n",
    "        (np.ones(len(df)), (user_codes, item_codes)),\n",
    "        shape=(len(unique_users), len(unique_items))\n",
    "    )\n",
    "\n",
    "    return interaction_matrix, user_map, item_map, reverse_user_map, reverse_item_map\n",
    "\n",
    "class SimpleContentBased:\n",
    "    \"\"\"Content-based filtering using available features\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.tfidf_encoder = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "        self.vendor_features = None\n",
    "        self.vendor_ids = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def preprocess_text(self, text_series):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        processed = text_series.fillna('').astype(str)\n",
    "        processed = processed.str.lower()\n",
    "        processed = processed.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "        return processed\n",
    "\n",
    "    def prepare_features(self, vendor_data):\n",
    "        \"\"\"Prepare vendor features using available columns\"\"\"\n",
    "        print(\"Preparing content-based features...\")\n",
    "        vendor_data = vendor_data.copy()\n",
    "\n",
    "        # Check which columns are available\n",
    "        available_columns = vendor_data.columns.tolist()\n",
    "        print(f\"Available columns in vendor_data: {available_columns}\")\n",
    "\n",
    "        feature_components = []\n",
    "\n",
    "        # Handle text features - cuisine (if available)\n",
    "        if 'cuisine_origin' in vendor_data.columns:\n",
    "            cuisine_text = self.preprocess_text(vendor_data['cuisine_origin'])\n",
    "            cuisine_emb = self.tfidf_encoder.fit_transform(cuisine_text).toarray()\n",
    "            feature_components.append(cuisine_emb)\n",
    "            print(f\"Added cuisine embeddings: {cuisine_emb.shape}\")\n",
    "        else:\n",
    "            # Create dummy cuisine feature\n",
    "            dummy_cuisine = np.zeros((len(vendor_data), 50))\n",
    "            feature_components.append(dummy_cuisine)\n",
    "            print(\"Added dummy cuisine features\")\n",
    "\n",
    "        # Handle numerical features (check which ones exist)\n",
    "        numerical_features = []\n",
    "        potential_numerical = ['vendor_rating', 'num_products', 'total_order_value']\n",
    "\n",
    "        for feature in potential_numerical:\n",
    "            if feature in vendor_data.columns:\n",
    "                numerical_features.append(feature)\n",
    "\n",
    "        if numerical_features:\n",
    "            numerical_data = vendor_data[numerical_features].fillna(0).values\n",
    "            feature_components.append(numerical_data)\n",
    "            print(f\"Added numerical features {numerical_features}: {numerical_data.shape}\")\n",
    "        else:\n",
    "            # Add dummy numerical features\n",
    "            dummy_numerical = np.zeros((len(vendor_data), 2))\n",
    "            feature_components.append(dummy_numerical)\n",
    "            print(\"Added dummy numerical features\")\n",
    "\n",
    "        # Handle categorical features\n",
    "        categorical_features = []\n",
    "        potential_categorical = ['vendor_geohash', 'chain_id']\n",
    "\n",
    "        for feature in potential_categorical:\n",
    "            if feature in vendor_data.columns:\n",
    "                self.label_encoders[feature] = LabelEncoder()\n",
    "                encoded = self.label_encoders[feature].fit_transform(\n",
    "                    vendor_data[feature].fillna('unknown')\n",
    "                ).reshape(-1, 1)\n",
    "                feature_components.append(encoded)\n",
    "                categorical_features.append(feature)\n",
    "\n",
    "        if categorical_features:\n",
    "            print(f\"Added categorical features {categorical_features}\")\n",
    "\n",
    "        # Combine all features\n",
    "        if feature_components:\n",
    "            combined_features = np.concatenate(feature_components, axis=1)\n",
    "        else:\n",
    "            # Fallback: use vendor_id as simple feature\n",
    "            combined_features = np.arange(len(vendor_data)).reshape(-1, 1)\n",
    "\n",
    "        # Scale features\n",
    "        self.vendor_features = self.scaler.fit_transform(combined_features)\n",
    "        self.vendor_ids = vendor_data['vendor_id'].values\n",
    "        self.is_fitted = True\n",
    "\n",
    "        print(f\"Created {combined_features.shape[1]} features for {len(vendor_data)} vendors\")\n",
    "        return self.vendor_features\n",
    "\n",
    "    def get_similar_vendors(self, vendor_id, top_k=10):\n",
    "        \"\"\"Find similar vendors based on content features\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted yet. Call prepare_features first.\")\n",
    "\n",
    "        if vendor_id not in self.vendor_ids:\n",
    "            return []\n",
    "\n",
    "        vendor_idx = np.where(self.vendor_ids == vendor_id)[0][0]\n",
    "        vendor_vector = self.vendor_features[vendor_idx].reshape(1, -1)\n",
    "\n",
    "        # Calculate similarities to all vendors\n",
    "        similarities = cosine_similarity(vendor_vector, self.vendor_features)[0]\n",
    "\n",
    "        # Get top similar vendors (excluding itself)\n",
    "        similar_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "\n",
    "        recommendations = []\n",
    "        for idx in similar_indices:\n",
    "            recommendations.append({\n",
    "                'vendor_id': self.vendor_ids[idx],\n",
    "                'similarity_score': similarities[idx]\n",
    "            })\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def get_user_content_scores(self, user_preferred_vendors, top_k=20):\n",
    "        \"\"\"Get content-based scores for a user based on their preferred vendors\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return {}\n",
    "\n",
    "        scores = {}\n",
    "\n",
    "        # For each preferred vendor, find similar ones and accumulate scores\n",
    "        for vendor_id in user_preferred_vendors[:3]:  # Use top 3 preferred vendors\n",
    "            try:\n",
    "                similar_vendors = self.get_similar_vendors(vendor_id, top_k=top_k)\n",
    "                for rec in similar_vendors:\n",
    "                    vendor_id_rec = rec['vendor_id']\n",
    "                    similarity = rec['similarity_score']\n",
    "\n",
    "                    # Take the maximum similarity score from any preferred vendor\n",
    "                    if vendor_id_rec not in scores or similarity > scores[vendor_id_rec]:\n",
    "                        scores[vendor_id_rec] = similarity\n",
    "            except Exception as e:\n",
    "                continue  # Skip if vendor not found or other error\n",
    "\n",
    "        return scores\n",
    "\n",
    "class HybridRecommendationModel:\n",
    "    \"\"\"Hybrid model combining collaborative filtering and content-based filtering\"\"\"\n",
    "\n",
    "    def __init__(self, cf_weight=0.7, content_weight=0.3):\n",
    "        self.cf_model = None\n",
    "        self.content_model = SimpleContentBased()\n",
    "        self.user_profiles = {}\n",
    "        self.cf_weight = cf_weight\n",
    "        self.content_weight = content_weight\n",
    "        self.user_map = None\n",
    "        self.item_map = None\n",
    "        self.reverse_user_map = None\n",
    "        self.reverse_item_map = None\n",
    "\n",
    "    def fit(self, train_data, vendor_data, val_data=None):\n",
    "        \"\"\"Train the hybrid model\"\"\"\n",
    "        print(\"Training Hybrid Recommendation Model...\")\n",
    "\n",
    "        # 1. Build user profiles (with robust column handling)\n",
    "        self._build_user_profiles(train_data)\n",
    "\n",
    "        # 2. Train Collaborative Filtering\n",
    "        print(\"Training Collaborative Filtering (BPR)...\")\n",
    "        self._train_collaborative_filtering(train_data)\n",
    "\n",
    "        # 3. Train Content-Based Model\n",
    "        print(\"Training Content-Based Model...\")\n",
    "        self.content_model.prepare_features(vendor_data)\n",
    "\n",
    "        print(\"Training completed!\")\n",
    "\n",
    "    def _build_user_profiles(self, train_data):\n",
    "        \"\"\"Build user preference profiles with robust column handling\"\"\"\n",
    "        print(\"Building user profiles...\")\n",
    "\n",
    "        # Check available columns\n",
    "        available_columns = train_data.columns.tolist()\n",
    "        print(f\"Available columns in train_data: {available_columns}\")\n",
    "\n",
    "        # User order statistics - only use available columns\n",
    "        agg_dict = {'vendor_id': 'count'}  # Always count vendor interactions\n",
    "\n",
    "        # Add available numerical columns\n",
    "        if 'vendor_rating' in available_columns:\n",
    "            agg_dict['vendor_rating'] = 'mean'\n",
    "        if 'total_order_value' in available_columns:\n",
    "            agg_dict['total_order_value'] = ['mean', 'sum']\n",
    "\n",
    "        user_stats = train_data.groupby('customer_id').agg(agg_dict)\n",
    "\n",
    "        # Flatten multi-level columns if they exist\n",
    "        if isinstance(user_stats.columns, pd.MultiIndex):\n",
    "            user_stats.columns = ['_'.join(col).strip() for col in user_stats.columns]\n",
    "        user_stats = user_stats.reset_index()\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        column_rename = {\n",
    "            'vendor_id_count': 'order_count',\n",
    "            'vendor_rating_mean': 'avg_rating',\n",
    "            'total_order_value_mean': 'avg_order_value',\n",
    "            'total_order_value_sum': 'total_spent'\n",
    "        }\n",
    "        user_stats = user_stats.rename(columns={k: v for k, v in column_rename.items() if k in user_stats.columns})\n",
    "\n",
    "        # Preferred cuisines (if available)\n",
    "        if 'cuisine_origin' in available_columns:\n",
    "            user_cuisines = train_data.groupby(['customer_id', 'cuisine_origin']).size().reset_index(name='count')\n",
    "            user_top_cuisines = user_cuisines.sort_values(['customer_id', 'count'], ascending=[True, False])\n",
    "            user_top_cuisines = user_top_cuisines.groupby('customer_id').head(3)\n",
    "        else:\n",
    "            user_top_cuisines = pd.DataFrame(columns=['customer_id', 'cuisine_origin'])\n",
    "\n",
    "        for user_id in train_data['customer_id'].unique():\n",
    "            user_orders = train_data[train_data['customer_id'] == user_id]\n",
    "            preferred_vendors = user_orders['vendor_id'].unique().tolist()\n",
    "\n",
    "            # Get user's preferred cuisines (if available)\n",
    "            user_cuisine_data = user_top_cuisines[user_top_cuisines['customer_id'] == user_id]\n",
    "            preferred_cuisines = user_cuisine_data['cuisine_origin'].tolist() if 'cuisine_origin' in user_cuisine_data.columns else []\n",
    "\n",
    "            # Get user stats\n",
    "            user_stat_data = user_stats[user_stats['customer_id'] == user_id]\n",
    "            avg_rating = user_stat_data['avg_rating'].iloc[0] if 'avg_rating' in user_stat_data.columns and len(user_stat_data) > 0 else 0\n",
    "            order_count = user_stat_data['order_count'].iloc[0] if 'order_count' in user_stat_data.columns and len(user_stat_data) > 0 else len(user_orders)\n",
    "\n",
    "            self.user_profiles[user_id] = {\n",
    "                'preferred_vendors': preferred_vendors,\n",
    "                'order_count': order_count,\n",
    "                'preferred_cuisines': preferred_cuisines,\n",
    "                'avg_rating': avg_rating\n",
    "            }\n",
    "\n",
    "        print(f\"Built profiles for {len(self.user_profiles)} users\")\n",
    "\n",
    "    def _train_collaborative_filtering(self, train_data):\n",
    "        \"\"\"Train BPR model for collaborative filtering\"\"\"\n",
    "        # Create interaction matrix with proper mappings\n",
    "        train_matrix, self.user_map, self.item_map, self.reverse_user_map, self.reverse_item_map = create_interaction_matrix(train_data)\n",
    "\n",
    "        # Train BPR model\n",
    "        self.cf_model = BayesianPersonalizedRanking(\n",
    "            factors=64,\n",
    "            learning_rate=0.01,\n",
    "            regularization=0.1,\n",
    "            iterations=30,  # Reduced for faster training\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        self.cf_model.fit(train_matrix)\n",
    "        print(f\"Trained CF model on {train_matrix.shape[0]} users and {train_matrix.shape[1]} items\")\n",
    "\n",
    "    def _get_cf_scores(self, user_id):\n",
    "        \"\"\"Get collaborative filtering scores for a user\"\"\"\n",
    "        if user_id not in self.reverse_user_map:\n",
    "            return {}\n",
    "\n",
    "        user_idx = self.reverse_user_map[user_id]\n",
    "\n",
    "        scores = {}\n",
    "\n",
    "        # Iterate through all items in the item_map (which uses integer codes as keys)\n",
    "        for item_code, vendor_id in self.item_map.items():\n",
    "            if item_code < len(self.cf_model.item_factors):\n",
    "                try:\n",
    "                    vendor_vector = self.cf_model.item_factors[item_code]\n",
    "                    user_vector = self.cf_model.user_factors[user_idx]\n",
    "                    score = np.dot(user_vector, vendor_vector)\n",
    "                    scores[vendor_id] = score\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def recommend(self, user_id, top_k=10):\n",
    "        \"\"\"Generate recommendations for a user\"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            # Cold start: return popular items\n",
    "            return self._get_popular_items(top_k)\n",
    "\n",
    "        # Get CF scores\n",
    "        cf_scores = self._get_cf_scores(user_id)\n",
    "\n",
    "        # Get content-based scores\n",
    "        preferred_vendors = self.user_profiles[user_id]['preferred_vendors']\n",
    "        content_scores = self.content_model.get_user_content_scores(preferred_vendors, top_k=30)\n",
    "\n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "        all_vendors = set(cf_scores.keys()) | set(content_scores.keys())\n",
    "\n",
    "        for vendor in all_vendors:\n",
    "            cf_score = cf_scores.get(vendor, 0)\n",
    "            content_score = content_scores.get(vendor, 0)\n",
    "\n",
    "            # Normalize scores\n",
    "            max_cf = max(cf_scores.values()) if cf_scores else 1\n",
    "            max_content = max(content_scores.values()) if content_scores else 1\n",
    "\n",
    "            cf_score_norm = cf_score / max_cf if max_cf > 0 else 0\n",
    "            content_score_norm = content_score / max_content if max_content > 0 else 0\n",
    "\n",
    "            combined_score = (self.cf_weight * cf_score_norm) + (self.content_weight * content_score_norm)\n",
    "            combined_scores[vendor] = combined_score\n",
    "\n",
    "        # Return top-k vendors\n",
    "        top_vendors = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return [vendor for vendor, score in top_vendors]\n",
    "\n",
    "    def _get_popular_items(self, top_k):\n",
    "        \"\"\"Get popular vendors for cold start\"\"\"\n",
    "        if hasattr(self, 'train_data'):\n",
    "            popular_vendors = self.train_data['vendor_id'].value_counts().head(top_k).index.tolist()\n",
    "            return popular_vendors\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def evaluate(self, test_data, k=10):\n",
    "        \"\"\"Comprehensive evaluation on test data\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        mrr_scores = []\n",
    "\n",
    "        evaluated_users = 0\n",
    "\n",
    "        for user_id in test_data['customer_id'].unique():\n",
    "            user_test_data = test_data[test_data['customer_id'] == user_id]\n",
    "            true_vendors = user_test_data['vendor_id'].tolist()\n",
    "\n",
    "            if user_id in self.user_profiles:\n",
    "                try:\n",
    "                    pred_vendors = self.recommend(user_id, top_k=k)\n",
    "\n",
    "                    if len(true_vendors) > 0 and len(pred_vendors) > 0:\n",
    "                        # Calculate NDCG\n",
    "                        ndcg = self._calculate_ndcg(true_vendors, pred_vendors, k)\n",
    "                        ndcg_scores.append(ndcg)\n",
    "\n",
    "                        # Calculate Precision\n",
    "                        precision = len(set(true_vendors) & set(pred_vendors)) / len(pred_vendors)\n",
    "                        precision_scores.append(precision)\n",
    "\n",
    "                        # Calculate Recall\n",
    "                        recall = len(set(true_vendors) & set(pred_vendors)) / len(true_vendors)\n",
    "                        recall_scores.append(recall)\n",
    "\n",
    "                        # Calculate MRR\n",
    "                        mrr = 0\n",
    "                        for i, vendor in enumerate(pred_vendors):\n",
    "                            if vendor in true_vendors:\n",
    "                                mrr = 1 / (i + 1)\n",
    "                                break\n",
    "                        mrr_scores.append(mrr)\n",
    "\n",
    "                        evaluated_users += 1\n",
    "                except Exception as e:\n",
    "                    # Skip users that cause errors\n",
    "                    continue\n",
    "\n",
    "        print(f\"Evaluated on {evaluated_users} users\")\n",
    "\n",
    "        results = {\n",
    "            'ndcg': np.mean(ndcg_scores) if ndcg_scores else 0,\n",
    "            'precision': np.mean(precision_scores) if precision_scores else 0,\n",
    "            'recall': np.mean(recall_scores) if recall_scores else 0,\n",
    "            'mrr': np.mean(mrr_scores) if mrr_scores else 0\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _calculate_ndcg(self, true_list, pred_list, k=10):\n",
    "        \"\"\"Calculate NDCG@k manually\"\"\"\n",
    "        # Binary relevance (1 if vendor in true list, 0 otherwise)\n",
    "        relevance = [1 if item in true_list else 0 for item in pred_list[:k]]\n",
    "\n",
    "        # Calculate DCG\n",
    "        dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)])\n",
    "\n",
    "        # Calculate IDCG (ideal ordering)\n",
    "        ideal_relevance = [1] * min(len(true_list), k) + [0] * max(0, k - len(true_list))\n",
    "        idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])\n",
    "\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def run_complete_pipeline(order_data, vendor_data, test_size=0.2):\n",
    "    \"\"\"Run the complete training and evaluation pipeline\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"FOOD DELIVERY RECOMMENDATION SYSTEM\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Split the data\n",
    "    print(\"\\n1. Splitting data...\")\n",
    "    splits = robust_temporal_split(order_data, test_size=test_size)\n",
    "    train_data, val_data, test_data = splits['train'], splits['val'], splits['test']\n",
    "\n",
    "    print(f\"Training set: {len(train_data)} orders\")\n",
    "    print(f\"Validation set: {len(val_data)} orders\")\n",
    "    print(f\"Test set: {len(test_data)} orders\")\n",
    "    print(f\"Unique users: {order_data['customer_id'].nunique()}\")\n",
    "    print(f\"Unique vendors: {order_data['vendor_id'].nunique()}\")\n",
    "\n",
    "    # 2. Train the hybrid model\n",
    "    print(\"\\n2. Training hybrid model...\")\n",
    "    hybrid_model = HybridRecommendationModel(cf_weight=0.7, content_weight=0.3)\n",
    "    hybrid_model.train_data = train_data  # Store for popular items\n",
    "\n",
    "    hybrid_model.fit(\n",
    "        train_data=train_data,\n",
    "        vendor_data=vendor_data\n",
    "    )\n",
    "\n",
    "    # 3. Evaluate on test set\n",
    "    print(\"\\n3. Evaluating on test set...\")\n",
    "    test_metrics = hybrid_model.evaluate(test_data, k=10)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric.upper()}@10: {value:.4f}\")\n",
    "\n",
    "    # 4. Compare with benchmark\n",
    "    benchmark = {\n",
    "        'ndcg': 0.6620,\n",
    "        'precision': 0.0662,\n",
    "        'mrr': 0.6620\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"BENCHMARK COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    for metric, value in test_metrics.items():\n",
    "        if metric in benchmark:\n",
    "            bench_value = benchmark[metric]\n",
    "            improvement = ((value - bench_value) / bench_value) * 100\n",
    "            status = \"✅ ABOVE\" if value >= bench_value else \"❌ BELOW\"\n",
    "            print(f\"{metric.upper()}@10: {value:.4f} vs {bench_value:.4f} | {status} | {improvement:+.1f}%\")\n",
    "\n",
    "    return hybrid_model, test_metrics\n",
    "\n",
    "# Run the corrected pipeline\n",
    "print(\"Starting the corrected pipeline...\")\n",
    "model, metrics = run_complete_pipeline(order_level_data, full_data2)\n",
    "\n",
    "# Generate recommendations for a sample user\n",
    "if model:\n",
    "    sample_user = order_level_data['customer_id'].iloc[0]\n",
    "    if sample_user in model.user_profiles:\n",
    "        recommendations = model.recommend(sample_user, top_k=5)\n",
    "        print(f\"\\nSample recommendations for user {sample_user}:\")\n",
    "        print(recommendations)\n",
    "    else:\n",
    "        print(f\"\\nSample user {sample_user} not in training data (cold start)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-delivery-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
